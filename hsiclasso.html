<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="members.html">Members</a></div>
<div class="menu-item"><a href="software.html">Softwares</a></div>
</td>
<td id="layout-content">
<p><br /></p>
<h1>Hilbert-Schmidt Independence Criterion Lasso (HSIC Lasso)</h1>
<h2>Introduction</h2>
<p>The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this project, we consider a
feature-wise kernelized Lasso for capturing non-linear input-output dependency. We first show that, with particular choices of kernel functions, non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems.</p>
<h2>Main Idea</h2>
<p>The HSIC Lasso is given as the following form</p>
<div class="infoblock">
<div class="blockcontent">
<p>\(\min_{\alpha_1,\ldots,\alpha_d}&nbsp;\frac{1}{2}\|\bar{\bf L} - \sum_{k = 1}^d \alpha_k \bar{\bf K}^{(k)}\|^2_{F} + \lambda \sum_{k = 1}^d |\alpha_k| \hspace{.3cm} \text{s.t.}&nbsp;\alpha_1,\ldots,\alpha_d \geq 0\)</p>
</div></div>
<p>where \(\|\cdot\|_F\) is the Frobenius norm, \(\bar{\bf K}^{(k)}\) is the centered Gram matrix computed from \(k\)-th feature, and \(\bar{\bf L}\) is the centered Gram matrix computed from output \(y\).</p>
<p>To compute the solutions of HSIC Lasso, we use the <a href="http://www.ibis.t.u-tokyo.ac.jp/ryotat/dal/" target=&ldquo;blank&rdquo;>dual augmented Lagrangian (DAL) package</a>.</p>
<h2>Features</h2>
<ul>
<li><p>Can select nonlinearly related features.</p>
</li>
<li><p>Highly scalable w.r.t. the number of features.</p>
</li>
<li><p>Convex optimization.</p>
</li>
</ul>
<h2>Download </h2>
<ul>
<li><p><a href="software/HSICLasso.zip" target=&ldquo;blank&rdquo;>HSIC Lasso (Matlab)</a></p>
</li>
<li><p><a href="software/HSICLasso_large.zip" target=&ldquo;blank&rdquo;>HSIC Lasso (Matlab, less memory implementation)</a>

</p>
</li>
<li><p><a href="https://github.com/riken-aip/pyHSICLasso" target=&ldquo;blank&rdquo;>HSIC Lasso (Python, LARS based approach)</a></p>
</li>
</ul>
<h2>Usage</h2>
<ul>
<li><p>Download the source code.</p>
</li>
<li><p>For the less memory implementation, you need to download <a href="http://bitbucket.org/eigen/eigen/get/3.0.7.zip" target=&ldquo;blank&rdquo;>eigen</a> and place it to the same folder of HSICLasso. Then, compile cpp files with mex.</p>
</li>
<li><p>Run the script (demo_HSICLasso.m).</p>
</li>
</ul>
<h2>Acknowledgement</h2>
<p>I am grateful to Prof. <a href="http://sugiyama-www.cs.titech.ac.jp/~sugi/" target=&ldquo;blank&rdquo;>Masashi Sugiyama</a> and Dr. <a href="http://cs.brown.edu/~ls/" target=&ldquo;blank&rdquo;>Leonid Sigal</a> for their support in developing this software.</p>
<h2>Contact</h2>
<p>I am happy to have any kind of feedbacks. E-mail: \(\texttt{makoto.yamada@riken.jp}\)</p>
<h2>Reference</h2>
<ul>
<li><p><b>Yamada, M.</b>, Jitkrittum, W., <a href="http://www.cs.brown.edu/~ls/" target=&ldquo;blank&rdquo;>Sigal, L.</a>, <a href="http://www.cs.cmu.edu/~epxing/" target=&ldquo;blank&rdquo;>Xing, E. P.</a> &amp; <a href="http://sugiyama-www.cs.titech.ac.jp/~sugi/" target=&ldquo;blank&rdquo;>Sugiyama, M.</a> <br />
High-Dimensional Feature Selection by Feature-Wise Non-Linear Lasso. <br />
<a href="http://www.mitpressjournals.org/loi/neco" target=&ldquo;blank&rdquo;>Neural Computation</a>, vol.26, no.1, pp.185-207, 2014. [<a href="http://arxiv.org/pdf/1202.0515.pdf" target=&ldquo;blank&rdquo;>paper</a>] [<a href="hsiclasso.html" target=&ldquo;blank&rdquo;>software</a>]</p>
</li>
<li><p><b>Yamada, M.</b>, <a href="http://www.brl.ntt.co.jp/people/akisato/index.html" target=&ldquo;blank&rdquo;>Kimura, A.</a>, Naya, F., &amp; <a href="http://www.kecl.ntt.co.jp/icl/signal/sawada/" target=&ldquo;blank&rdquo;>Sawada, H</a>. <br />
Change-Point Detection with Feature Selection in High-dimensional Time-Series Data. <br />
In Proceedings of <a href="http://ijcai13.org/" target=&ldquo;blank&rdquo;>International Joint Conference on Artificial Intelligence (IJCAI 2013)</a>, pp. 1827-1833.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2020-03-11 08:09:58 JST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
(<a href="hsiclasso.jemdoc">source</a>)
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
